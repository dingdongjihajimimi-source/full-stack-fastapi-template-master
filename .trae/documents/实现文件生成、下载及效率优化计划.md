# 后端优化与文件生成功能实现计划

## 1. 目录结构设计

将在后端 `backend` 目录下创建统一的数据生成目录 `generated_data`，结构如下：

```text
backend/generated_data/
├── csv/   # 存放爬虫原始数据 (.csv)
└── sql/   # 存放生成的 SQL 脚本 (.sql)
```

## 2. 后端逻辑改造 (backend)

### 2.1 效率优化与文件生成 (`worker_tasks/crawler.py`)

* **并发优化**：将原本**串行**的 DeepSeek API 调用重构为基于 `asyncio.gather` 的**并发**调用，显著提升 SQL 生成速度。

* **CSV 生成**：在获取/模拟数据后，立即将其写入 `generated_data/csv/{task_id}.csv`。

* **SQL 文件生成**：在 SQL 生成完毕后，将完整内容写入 `generated_data/sql/{task_id}.sql`。

* **自动建目录**：代码中增加检测并创建上述目录的逻辑。

### 2.2 新增下载接口 (`api/routes/crawler.py`)

* 新增 API 端点：`GET /crawl/download/{task_id}/{file_type}`

  * `file_type` 参数支持 `csv` 或 `sql`。

  * 使用 `FileResponse` 返回文件，支持浏览器直接下载。

## 3. 前端界面优化 (frontend)

### 3.1 界面交互 (`routes/_layout/crawler.tsx`)

* **新增下载按钮**：在任务完成后的结果展示区域，增加“📥 下载 CSV 数据”和“📥 下载 SQL 文件”两个按钮。

* **下载逻辑**：实现前端调用下载接口并触发浏览器下载行为的逻辑。

## 4. 验证计划

* 运行爬虫任务，验证 `generated_data` 目录是否生成了对应的 `.csv` 和 `.sql` 文件。

* 观察任务执行时间，确认并发优化是否生效（预期速度大幅提升）。

* 点击前端下载按钮，验证文件是否能正确下载且内容无误。

运行build指令重新部署加载docker的项目

