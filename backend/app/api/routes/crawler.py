import uuid
import json
import logging
from datetime import datetime
from typing import Any, Optional, Dict

from fastapi import APIRouter, BackgroundTasks, HTTPException
from fastapi.responses import FileResponse
from sqlmodel import select
from pydantic import BaseModel

from app.api.deps import SessionDep
from app.models import CrawlerTask
from app.worker_tasks.crawler import generate_sql_from_spider, CSV_DIR, SQL_DIR
from app.sniffer_pipeline.pipeline import SnifferPipeline
from app.sniffer_pipeline.schemas import ExtractionStrategy
from app.core.paths import CSV_DIR, SQL_DIR
router = APIRouter()
logger = logging.getLogger(__name__)

class CrawlRequest(BaseModel):
    url: str
    table_name: Optional[str] = None
    columns: list[str] = []
    max_pages: int = 1
    concurrency: int = 5
    mode: str = "manual"  # "manual" æˆ– "auto"
    review_mode: bool = False # è‡ªåŠ¨æ¨¡å¼ä¸‹æš‚åœç­‰å¾…å®¡æ ¸

class ResumeRequest(BaseModel):
    task_id: uuid.UUID
    strategy: dict # The confirmed/edited strategy

@router.post("/start", response_model=uuid.UUID)
def start_crawl(
    request: CrawlRequest,
    background_tasks: BackgroundTasks,
    session: SessionDep,
) -> Any:
    """
    å¯åŠ¨çˆ¬è™«ä»»åŠ¡ï¼ˆæ‰‹åŠ¨æˆ–è‡ªä¸»ï¼‰ã€‚
    """
    crawler_task = CrawlerTask(status="pending")
    session.add(crawler_task)
    session.commit()
    session.refresh(crawler_task)

    if request.mode == "auto":
        # åˆå§‹åŒ–ç®¡é“çŠ¶æ€å¹¶è®°å½•å¯åŠ¨æ—¥å¿—
        initial_logs = [f"[{datetime.now().strftime('%H:%M:%S')}] ä»»åŠ¡åˆå§‹åŒ–ã€‚å·²æ’é˜Ÿç­‰å¾…æ‰§è¡Œ..."]
        crawler_task.pipeline_state = json.dumps({"logs": initial_logs})
        session.add(crawler_task)
        session.commit()

        background_tasks.add_task(
            run_autonomous_pipeline_task,
            str(crawler_task.id),
            request.url,
            request.table_name,
            request.review_mode
        )
    else:
        # å¦‚æœæœªæä¾›ï¼Œåˆ™å›é€€åˆ°æ‰‹åŠ¨æ¨¡å¼é»˜è®¤å€¼
        table_name = request.table_name or "scraped_data"
        columns = request.columns or ["content"]
        
        background_tasks.add_task(
            generate_sql_from_spider, 
            crawler_task.id, 
            request.url, 
            table_name, 
            columns,
            request.max_pages,
            request.concurrency
        )

    return crawler_task.id

@router.post("/resume", response_model=Dict[str, str])
def resume_crawl(
    request: ResumeRequest,
    background_tasks: BackgroundTasks,
    session: SessionDep,
) -> Any:
    """
    ä½¿ç”¨ç¡®è®¤çš„ç­–ç•¥æ¢å¤æš‚åœçš„è‡ªä¸»çˆ¬å–ã€‚
    """
    task = session.get(CrawlerTask, request.task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    if task.status != "paused":
        raise HTTPException(status_code=400, detail="Task is not paused")

    # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
    task.status = "processing"
    session.add(task)
    session.commit()

    # ä»å­—å…¸é‡æ–°å®ä¾‹åŒ–ç­–ç•¥
    try:
        strategy = ExtractionStrategy(**request.strategy)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid strategy: {e}")

    background_tasks.add_task(
        resume_autonomous_pipeline_task,
        str(task.id),
        strategy
    )

    return {"status": "resumed"}

async def run_autonomous_pipeline_task(
    task_id: str, 
    url: str, 
    table_name_hint: Optional[str],
    review_mode: bool
):
    """
    è¿è¡Œç®¡é“å¹¶æ›´æ–°æ•°æ®åº“çŠ¶æ€çš„åŒ…è£…å™¨ã€‚
    æˆ‘ä»¬åœ¨è¿™é‡Œéœ€è¦ä¸€ä¸ªæ–°çš„ä¼šè¯ï¼Œå› ä¸ºå¦‚æœåœ¨åå°è¿è¡Œï¼Œ
    FastAPI BackgroundTasks é€šå¸¸ä¼šå…±äº«ä¼šè¯ä¸Šä¸‹æ–‡ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ã€‚
    å®é™…ä¸Šï¼Œè¿™é‡Œä¼ é€’çš„ `session` å¯èƒ½ä¼šåœ¨è¯·æ±‚ç»“æŸæ—¶å…³é—­ã€‚
    æˆ‘ä»¬åº”è¯¥åˆ›å»ºä¸€ä¸ªæ–°çš„ä¼šè¯æˆ–åœ¨å®‰å…¨çš„æƒ…å†µä¸‹ä½¿ç”¨ä¼ é€’çš„ä¼šè¯ã€‚
    FastAPI æ–‡æ¡£ç§° BackgroundTasks åœ¨å“åº”åè¿è¡Œï¼Œå› æ­¤ä¾èµ–ä¼šè¯å¯èƒ½ä¼šå…³é—­ã€‚
    æˆ‘ä»¬åº”è¯¥ä½¿ç”¨æ–°çš„ä¼šè¯å·¥å‚ã€‚
    """
    from app.core.db import engine
    from sqlmodel import Session
    
    pipeline = SnifferPipeline()

    async def update_state(tid, phase, data):
        with Session(engine) as db_session:
            task = db_session.get(CrawlerTask, uuid.UUID(tid))
            if task:
                task.current_phase = phase
                existing = json.loads(task.pipeline_state) if task.pipeline_state else {}
                
                # æ›´æ–°æ—¥å¿—
                logs = existing.get("logs", [])
                timestamp = datetime.now().strftime("%H:%M:%S")
                
                # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰ç‰¹å®šçš„æ—¥å¿—æ¶ˆæ¯
                log_message = f"Phase: {phase}"
                if data and "log_message" in data:
                    log_message = data["log_message"]
                    # å¦‚æœæ˜¯çº¯æ—¥å¿—æ›´æ–°ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³æ›´æ”¹æ•°æ®åº“ä¸­çš„é˜¶æ®µ
                    # ä½† current_phase å¯¹ UI è¿›åº¦æ¡å¾ˆæœ‰ç”¨ã€‚
                    # å¦‚æœé˜¶æ®µæ˜¯â€œæ—¥å¿—â€ï¼Œæˆ‘ä»¬ä¿ç•™ä¸Šä¸€ä¸ªé˜¶æ®µï¼Ÿ
                    # è®©æˆ‘ä»¬å‡è®¾é˜¶æ®µæ€»æ˜¯æ­£ç¡®ä¼ é€’çš„ã€‚
                elif phase == "scout":
                     log_message = "é˜¶æ®µï¼šä¾¦å¯Ÿï¼ˆé‡‡æ ·ï¼‰"
                elif phase == "architect":
                     log_message = "é˜¶æ®µï¼šæ¶æ„å¸ˆï¼ˆç­–ç•¥å®šä¹‰ï¼‰"
                elif phase == "review":
                     log_message = "é˜¶æ®µï¼šå®¡æ ¸ï¼ˆç­‰å¾…ç”¨æˆ·ï¼‰"
                elif phase == "harvester":
                     log_message = "é˜¶æ®µï¼šæ”¶è·è€…ï¼ˆæ‰§è¡Œï¼‰"
                elif phase == "refinery":
                     log_message = "é˜¶æ®µï¼šç²¾ç‚¼å‚ï¼ˆETL & SQLï¼‰"
                elif phase == "completed":
                     log_message = "é˜¶æ®µï¼šå·²å®Œæˆ"
                elif phase == "failed":
                     error_msg = data.get("error", "æœªçŸ¥é”™è¯¯") if data else "æœªçŸ¥é”™è¯¯"
                     log_message = f"é˜¶æ®µï¼šå¤±è´¥ - {error_msg}"

                logs.append(f"[{timestamp}] {log_message}")
                existing["logs"] = logs
                
                if data:
                    # å¦‚æœä¸å­˜åœ¨ URL åˆ™æ·»åŠ ï¼ˆç”¨äºæ¢å¤çš„ä¸´æ—¶å¤„ç†ï¼‰
                    if "url" not in existing:
                        existing["url"] = url
                    existing.update(data)
                
                task.pipeline_state = json.dumps(existing)
                
                if phase == "completed":
                    task.status = "completed"
                    if data and "items_harvested" in data:
                        existing["items_harvested"] = data["items_harvested"]
                elif phase == "failed":
                    task.status = "failed"
                elif phase == "review":
                    task.status = "paused"
                else:
                    task.status = "processing"
                
                db_session.add(task)
                db_session.commit()

    # è¿è¡Œç®¡é“
    await pipeline.run(url, task_id, update_callback=update_state, table_name_hint=table_name_hint, review_mode=review_mode)

async def resume_autonomous_pipeline_task(
    task_id: str,
    strategy: ExtractionStrategy
):
    from app.core.db import engine
    from sqlmodel import Session
    
    logger.info(f"ğŸ”„ Resuming autonomous pipeline task: {task_id}")

    # Retrieve URL from saved state
    # ä»ä¿å­˜çš„çŠ¶æ€ä¸­æ£€ç´¢ URL
    url = ""
    with Session(engine) as db_session:
        task = db_session.get(CrawlerTask, uuid.UUID(task_id))
        if task and task.pipeline_state:
            state = json.loads(task.pipeline_state)
            url = state.get("url", "")
    
    if not url:
        logger.error(f"Could not find URL for resuming task {task_id}")
        return

    pipeline = SnifferPipeline()

    async def update_state(tid, phase, data):
        with Session(engine) as db_session:
            task = db_session.get(CrawlerTask, uuid.UUID(tid))
            if task:
                task.current_phase = phase
                existing = json.loads(task.pipeline_state) if task.pipeline_state else {}
                
                # Update logs
                logs = existing.get("logs", [])
                timestamp = datetime.now().strftime("%H:%M:%S")
                
                # Check if there is a specific log message in data
                log_message = f"Phase: {phase}"
                if data and "log_message" in data:
                    log_message = data["log_message"]
                elif phase == "scout":
                     log_message = "Phase: Scout (Sampling)"
                elif phase == "architect":
                     log_message = "Phase: Architect (Strategy Definition)"
                elif phase == "review":
                     log_message = "Phase: Review (Waiting for user)"
                elif phase == "harvester":
                     log_message = "Phase: Harvester (Execution)"
                elif phase == "refinery":
                     log_message = "Phase: Refinery (ETL & SQL)"
                elif phase == "completed":
                     log_message = "Phase: Completed"
                elif phase == "failed":
                     error_msg = data.get("error", "Unknown error") if data else "Unknown error"
                     log_message = f"Phase: Failed - {error_msg}"

                logs.append(f"[{timestamp}] {log_message}")
                existing["logs"] = logs

                if data:
                    existing.update(data)
                
                task.pipeline_state = json.dumps(existing)
                
                if phase == "completed":
                    task.status = "completed"
                elif phase == "failed":
                    task.status = "failed"
                else:
                    task.status = "processing"
                
                db_session.add(task)
                db_session.commit()

    await pipeline.resume(task_id, url, strategy, update_callback=update_state)


@router.get("/{task_id}", response_model=CrawlerTask)
def get_crawl_status(
    task_id: uuid.UUID,
    session: SessionDep,
) -> Any:
    """
    è·å–çˆ¬è™«ä»»åŠ¡çŠ¶æ€ã€‚
    """
    task = session.get(CrawlerTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    return task

@router.get("/download/{task_id}/{file_type}")
def download_crawl_file(
    task_id: uuid.UUID,
    file_type: str,
    session: SessionDep,
) -> Any:
    """
    ä¸‹è½½ç”Ÿæˆçš„ CSV æˆ– SQL æ–‡ä»¶ã€‚
    """
    task = session.get(CrawlerTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    if file_type == "csv":
        file_path = CSV_DIR / f"{task_id}.csv"
        filename = f"crawler_data_{task_id}.csv"
        media_type = "text/csv"
    elif file_type == "sql":
        file_path = SQL_DIR / f"{task_id}.sql"
        filename = f"generated_sql_{task_id}.sql"
        media_type = "application/sql"
    else:
        raise HTTPException(status_code=400, detail="Invalid file type. Must be 'csv' or 'sql'.")
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found. Please ensure the task is completed.")

    return FileResponse(
        path=file_path, 
        filename=filename, 
        media_type=media_type,
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )

@router.get("/logs/{task_id}")
def get_task_logs(task_id: uuid.UUID, session: SessionDep):
    """
    ä» pipeline_state è·å–ä»»åŠ¡çš„æœ€è¿‘æ—¥å¿—/äº‹ä»¶ã€‚
    """
    task = session.get(CrawlerTask, task_id)
    if not task or not task.pipeline_state:
        return {"logs": []}
    
    state = json.loads(task.pipeline_state)
    # æˆ‘ä»¬å¯ä»¥åœ¨çŠ¶æ€ä¸­å­˜å‚¨â€œæ—¥å¿—â€åˆ—è¡¨
    logs = state.get("logs", [])
    return {"logs": logs}
